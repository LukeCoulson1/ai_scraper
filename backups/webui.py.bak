import pandas as pd
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from bs4 import BeautifulSoup
from llama_cpp import Llama
import time
from urllib.parse import urlparse
import re
import os
import io
import json
import tempfile
from tqdm import tqdm
import streamlit as st

try:
    from langdetect import detect
except ImportError:
    detect = None

PROMPT_TEMPLATES = {
    "Summarize": "Summarize the following content.",
    "Extract Table": "Extract all tables from the following content in markdown format.",
    "Translate to English": "Translate the following content to English.",
    "Extract Keywords": "Extract the most important keywords from the following content.",
    "Custom": None
}

def scrape_page(url):
    edge_options = Options()
    edge_options.add_argument("--headless")
    edge_options.add_argument("--disable-gpu")
    service = Service(r"F:\Python_Programs\Dependencies\edgedriver_win64\msedgedriver.exe")
    driver = webdriver.Edge(service=service, options=edge_options)
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()
    product_section = soup.find('div', class_='product-details')
    if not product_section:
        product_section = soup
    text = product_section.get_text(separator='\n', strip=True)
    return text, str(product_section)

def extract_html_tables(html):
    soup = BeautifulSoup(html, 'html.parser')
    tables = soup.find_all('table')
    dfs = []
    for table in tables:
        try:
            df = pd.read_html(str(table))[0]
            dfs.append(df)
        except Exception:
            continue
    return dfs

def gemma_llm_task(context, instruction, model_path, n_ctx=4096, max_tokens=1024):
    max_context_chars = 2000
    if len(context) > max_context_chars:
        context = context[:max_context_chars]
    llm = Llama(
        model_path=model_path,
        n_ctx=n_ctx,
        n_threads=8,
        n_gpu_layers=-1
    )
    prompt = f"{instruction}\n\nContent:\n{context}\n\nResponse:"
    output = llm(
        prompt,
        max_tokens=max_tokens,
        temperature=0.7,
        stop=["Response:"]
    )
    return output["choices"][0]["text"].strip()

def url_to_filename(url, ext="txt"):
    parsed = urlparse(url)
    domain = parsed.netloc.replace('.', '_')
    path = re.sub(r'[^a-zA-Z0-9]', '_', parsed.path.strip('/'))
    if path:
        filename = f"{domain}_{path}.{ext}"
    else:
        filename = f"{domain}.{ext}"
    return filename

def extract_markdown_tables(text, reference_columns=None):
    lines = text.splitlines()
    tables = []
    table_lines = []
    in_table = False
    for line in lines + [""]:
        if re.match(r'^\s*\|', line):
            in_table = True
            table_lines.append(line.strip())
        elif in_table and not line.strip():
            if table_lines:
                if len(table_lines) > 1 and re.match(r'^\s*\|[\s\-|]+\|$', table_lines[1]):
                    del table_lines[1]
                data = [row.strip('|').split('|') for row in table_lines]
                try:
                    if reference_columns is not None:
                        if len(data) > 1 and [c.strip() for c in data[0]] == reference_columns:
                            df = pd.DataFrame(data[1:], columns=reference_columns)
                        else:
                            df = pd.DataFrame(data, columns=reference_columns)
                    else:
                        columns = [c.strip() for c in data[0]]
                        df = pd.DataFrame(data[1:], columns=columns)
                    tables.append(df)
                except Exception:
                    pass
                table_lines = []
            in_table = False
    return tables

def chunk_text(text, max_chars=2000):
    for i in range(0, len(text), max_chars):
        yield text[i:i + max_chars]

def detect_language(text):
    if detect:
        try:
            return detect(text)
        except Exception:
            return "unknown"
    return "unknown"

def search_keywords(text, keywords):
    found = []
    for kw in keywords:
        if re.search(kw, text, re.IGNORECASE):
            found.append(kw)
    return found

def choose_model():
    model_dir = r"C:\Users\lukea\.lmstudio\models\lmstudio-community"
    models = [os.path.join(root, file)
              for root, _, files in os.walk(model_dir)
              for file in files if file.endswith(".gguf")]
    if not models:
        st.error(f"No models found in {model_dir}")
        st.stop()
    return st.selectbox("Choose LLM model", models)

def choose_prompt():
    prompt_keys = list(PROMPT_TEMPLATES.keys())
    idx = st.selectbox("Prompt template", range(len(prompt_keys)), format_func=lambda i: prompt_keys[i])
    key = prompt_keys[idx]
    if key == "Custom":
        return st.text_area("Enter your custom instruction:")
    return PROMPT_TEMPLATES[key]

def save_tables_download(tables, save_type, merge=True, separate_excel_sheets=False):
    if not tables:
        st.warning("No tables found to save.")
        return None, None
    if merge:
        final_df = pd.concat(tables, ignore_index=True).drop_duplicates()
        if save_type == "Excel":
            output = io.BytesIO()
            final_df.to_excel(output, index=False)
            return output.getvalue(), "result.xlsx"
        elif save_type == "CSV":
            return final_df.to_csv(index=False).encode(), "result.csv"
        elif save_type == "JSON":
            return final_df.to_json(orient="records", force_ascii=False).encode(), "result.json"
        elif save_type == "Markdown":
            return final_df.to_markdown(index=False).encode(), "result.md"
    elif separate_excel_sheets and save_type == "Excel":
        output = io.BytesIO()
        with pd.ExcelWriter(output) as writer:
            for idx, df in enumerate(tables):
                sheet_name = f"Table_{idx+1}"
                df.to_excel(writer, sheet_name=sheet_name, index=False)
        return output.getvalue(), "result_multi.xlsx"
    else:
        # Only support download of merged for non-Excel for simplicity
        st.warning("Separate download for non-Excel not supported in this demo.")
        return None, None

st.set_page_config(page_title="AI Scraper Web UI", layout="wide")
st.title("AI Scraper Web UI")

with st.sidebar:
    st.header("Configuration")
    model_path = choose_model()
    prompt_template = choose_prompt()
    save_type = st.selectbox("Save as", ["Excel", "CSV", "JSON", "Markdown"])
    merge_tables = st.checkbox("Merge all tables into one file", value=True)
    separate_excel_sheets = st.checkbox("Save each table as a separate Excel sheet (Excel only)", value=False)
    max_tokens = st.slider("Max tokens per chunk", 256, 4096, 1024, step=128)

url_input = st.text_area("Enter one or more URLs (comma or newline separated):")
run_button = st.button("Scrape and Process")

if run_button and url_input.strip():
    urls = [u.strip() for u in url_input.replace('\n', ',').split(",") if u.strip()]
    for url in urls:
        st.markdown(f"### Processing: {url}")
        try:
            with st.spinner("Scraping page..."):
                content, html = scrape_page(url)
            lang = detect_language(content)
            st.write(f"Detected language: {lang}")
            if lang != "en":
                if st.checkbox(f"Translate {url} content to English?"):
                    content = gemma_llm_task(content, PROMPT_TEMPLATES["Translate to English"], model_path, max_tokens=max_tokens)
            if st.checkbox(f"Summarize {url} content?"):
                summary = gemma_llm_task(content, PROMPT_TEMPLATES["Summarize"], model_path, max_tokens=max_tokens)
                st.info(summary)
            if st.checkbox(f"Extract keywords from {url}?"):
                keywords = gemma_llm_task(content, PROMPT_TEMPLATES["Extract Keywords"], model_path, max_tokens=max_tokens)
                st.info(keywords)
            user_keywords = st.text_input(f"Enter keywords or regex patterns to search in {url} (comma separated):", key=url)
            found = search_keywords(content, [k.strip() for k in user_keywords.split(",") if k.strip()]) if user_keywords else []
            if found:
                st.success("Found keywords/patterns: " + ", ".join(found))
            instruction = prompt_template
            all_tables = []
            responses = []
            reference_columns = None
            chunks = list(chunk_text(content, max_chars=2000))
            st.write(f"Processing {len(chunks)} chunks...")
            progress = st.progress(0)
            for idx, chunk in enumerate(chunks):
                response = gemma_llm_task(chunk, instruction, model_path, max_tokens=max_tokens)
                responses.append(response)
                tables = extract_markdown_tables(response, reference_columns)
                if tables and reference_columns is None:
                    reference_columns = tables[0].columns.tolist()
                all_tables.extend(tables)
                progress.progress((idx + 1) / len(chunks))
            if st.checkbox(f"Extract HTML tables from {url}?"):
                html_tables = extract_html_tables(html)
                all_tables.extend(html_tables)
            if all_tables:
                st.success(f"Extracted {len(all_tables)} tables.")
                st.dataframe(pd.concat(all_tables, ignore_index=True).head(100))
                file_bytes, file_name = save_tables_download(
                    all_tables, save_type, merge=merge_tables, separate_excel_sheets=separate_excel_sheets
                )
                if file_bytes:
                    st.download_button("Download result", file_bytes, file_name)
            else:
                st.warning("No tables found.")
            if st.checkbox(f"Show raw LLM responses for {url}?"):
                for i, resp in enumerate(responses):
                    st.text_area(f"Chunk {i+1} LLM Output", resp, height=150)
            if st.checkbox(f"Show raw HTML for {url}?"):
                st.text_area("HTML", html[:10000], height=300)
        except Exception as e:
            st.error(f"Error processing {url}: {e}")
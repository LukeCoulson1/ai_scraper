import pandas as pd
from selenium import webdriver
from selenium.webdriver.edge.service import Service
from selenium.webdriver.edge.options import Options
from bs4 import BeautifulSoup
from llama_cpp import Llama
import time
from urllib.parse import urlparse
import re
import os
import json
from tqdm import tqdm

try:
    from langdetect import detect
except ImportError:
    detect = None

PROMPT_TEMPLATES = {
    "Summarize": "Summarize the following content.",
    "Extract Table": "Extract all tables from the following content in markdown format.",
    "Translate to English": "Translate the following content to English.",
    "Extract Keywords": "Extract the most important keywords from the following content.",
    "Custom": None
}

def scrape_page(url):
    edge_options = Options()
    edge_options.add_argument("--headless")
    edge_options.add_argument("--disable-gpu")
    service = Service(r"F:\Python_Programs\Dependencies\edgedriver_win64\msedgedriver.exe")
    driver = webdriver.Edge(service=service, options=edge_options)
    driver.get(url)
    time.sleep(5)
    soup = BeautifulSoup(driver.page_source, 'html.parser')
    driver.quit()
    product_section = soup.find('div', class_='product-details')
    if not product_section:
        product_section = soup
    text = product_section.get_text(separator='\n', strip=True)
    return text, str(product_section)

def extract_html_tables(html):
    soup = BeautifulSoup(html, 'html.parser')
    tables = soup.find_all('table')
    dfs = []
    for table in tables:
        try:
            df = pd.read_html(str(table))[0]
            dfs.append(df)
        except Exception:
            continue
    return dfs

def gemma_llm_task(context, instruction, model_path, n_ctx=4096, max_tokens=1024, stream=False):
    max_context_chars = 2000
    if len(context) > max_context_chars:
        context = context[:max_context_chars]
    llm = Llama(
        model_path=model_path,
        n_ctx=n_ctx,
        n_threads=8,
        n_gpu_layers=-1
    )
    prompt = f"{instruction}\n\nContent:\n{context}\n\nResponse:"
    if stream:
        output = ""
        for chunk in llm.create_completion(prompt, max_tokens=max_tokens, temperature=0.7, stop=["Response:"], stream=True):
            print(chunk["choices"][0]["text"], end="", flush=True)
            output += chunk["choices"][0]["text"]
        print()
        return output.strip()
    else:
        output = llm(
            prompt,
            max_tokens=max_tokens,
            temperature=0.7,
            stop=["Response:"]
        )
        return output["choices"][0]["text"].strip()

def url_to_filename(url, ext="txt"):
    parsed = urlparse(url)
    domain = parsed.netloc.replace('.', '_')
    path = re.sub(r'[^a-zA-Z0-9]', '_', parsed.path.strip('/'))
    if path:
        filename = f"{domain}_{path}.{ext}"
    else:
        filename = f"{domain}.{ext}"
    return filename

def extract_markdown_tables(text):
    lines = text.splitlines()
    tables = []
    table_lines = []
    in_table = False
    for line in lines + [""]:
        if re.match(r'^\s*\|', line):
            in_table = True
            table_lines.append(line.strip())
        elif in_table and not line.strip():
            if table_lines:
                if len(table_lines) > 1 and re.match(r'^\s*\|[\s\-|]+\|$', table_lines[1]):
                    del table_lines[1]
                data = [row.strip('|').split('|') for row in table_lines]
                try:
                    df = pd.DataFrame(data[1:], columns=[c.strip() for c in data[0]])
                    tables.append(df)
                except Exception:
                    pass
                table_lines = []
            in_table = False
    return tables

def chunk_text(text, max_chars=2000):
    for i in range(0, len(text), max_chars):
        yield text[i:i + max_chars]

def detect_language(text):
    if detect:
        try:
            return detect(text)
        except Exception:
            return "unknown"
    return "unknown"

def search_keywords(text, keywords):
    found = []
    for kw in keywords:
        if re.search(kw, text, re.IGNORECASE):
            found.append(kw)
    return found

def log_error(msg):
    with open("scraper_errors.log", "a", encoding="utf-8") as f:
        f.write(msg + "\n")

def choose_model():
    model_dir = r"C:\Users\lukea\.lmstudio\models\lmstudio-community"
    models = [os.path.join(root, file)
              for root, _, files in os.walk(model_dir)
              for file in files if file.endswith(".gguf")]
    if not models:
        print("No models found in", model_dir)
        exit(1)
    print("Available models:")
    for i, m in enumerate(models):
        print(f"{i+1}: {m}")
    idx = int(input("Choose model number: ")) - 1
    return models[idx]

def choose_prompt():
    print("Prompt templates:")
    for i, k in enumerate(PROMPT_TEMPLATES):
        print(f"{i+1}: {k}")
    idx = int(input("Choose prompt template: ")) - 1
    key = list(PROMPT_TEMPLATES.keys())[idx]
    if key == "Custom":
        return input("Enter your custom instruction: ")
    return PROMPT_TEMPLATES[key]

def save_tables(tables, url, save_type, merge=True):
    base_filename = url_to_filename(url, "")
    if not tables:
        print("No tables found to save.")
        return
    if merge:
        final_df = pd.concat(tables, ignore_index=True).drop_duplicates()
        if save_type == "2":
            filename = base_filename + "xlsx"
            final_df.to_excel(filename, index=False)
        elif save_type == "3":
            filename = base_filename + "csv"
            final_df.to_csv(filename, index=False)
        elif save_type == "4":
            filename = base_filename + "json"
            final_df.to_json(filename, orient="records", force_ascii=False)
        elif save_type == "5":
            filename = base_filename + "md"
            final_df.to_markdown(filename, index=False)
        print(f"Combined table saved to {filename}\n")
    else:
        for idx, df in enumerate(tables):
            if save_type == "2":
                filename = f"{base_filename}table_{idx+1}.xlsx"
                df.to_excel(filename, index=False)
            elif save_type == "3":
                filename = f"{base_filename}table_{idx+1}.csv"
                df.to_csv(filename, index=False)
            elif save_type == "4":
                filename = f"{base_filename}table_{idx+1}.json"
                df.to_json(filename, orient="records", force_ascii=False)
            elif save_type == "5":
                filename = f"{base_filename}table_{idx+1}.md"
                df.to_markdown(filename, index=False)
            print(f"Table {idx+1} saved to {filename}")

if __name__ == "__main__":
    print("Welcome to the Advanced AI Scraper!")
    model_path = choose_model()
    prompt_template = choose_prompt()

    # Improved input mode selection
    mode = input("Choose input mode: (1) Direct URL(s), (2) Batch file [1/2]: ").strip()
    urls = []
    if mode == "2":
        file_path = input("Enter path to file with URLs (one per line): ").strip()
        with open(file_path, "r", encoding="utf-8") as f:
            urls = [line.strip() for line in f if line.strip()]
    else:
        url_input = input("Enter one or more URLs (comma or newline separated):\n").replace('\n', ',')
        urls = [u.strip() for u in url_input.split(",") if u.strip()]

    for url in urls:
        try:
            print(f"\nProcessing: {url}")
            content, html = scrape_page(url)
            lang = detect_language(content)
            print(f"Detected language: {lang}")
            if lang != "en":
                if input("Translate to English? (y/n): ").strip().lower() == "y":
                    content = gemma_llm_task(content, PROMPT_TEMPLATES["Translate to English"], model_path)
            if input("Summarize content? (y/n): ").strip().lower() == "y":
                summary = gemma_llm_task(content, PROMPT_TEMPLATES["Summarize"], model_path)
                print("\nSummary:\n", summary)
            if input("Extract keywords? (y/n): ").strip().lower() == "y":
                keywords = gemma_llm_task(content, PROMPT_TEMPLATES["Extract Keywords"], model_path)
                print("\nKeywords:\n", keywords)
            user_keywords = input("Enter keywords or regex patterns to search (comma separated, or leave blank): ").strip()
            found = search_keywords(content, [k.strip() for k in user_keywords.split(",") if k.strip()]) if user_keywords else []
            if found:
                print("Found keywords/patterns:", ", ".join(found))
            instruction = prompt_template
            stream = input("Stream LLM output to console? (y/n): ").strip().lower() == "y"
            save_type = input("Save as (1) Text, (2) Excel, (3) CSV, (4) JSON, (5) Markdown? Enter 1-5: ").strip()
            merge_tables = input("Merge all tables into one file? (y/n): ").strip().lower() == "y"
            all_tables = []
            responses = []
            chunks = list(chunk_text(content, max_chars=2000))
            print(f"Processing {len(chunks)} chunks...")
            for idx, chunk in enumerate(tqdm(chunks, desc="Chunks")):
                response = gemma_llm_task(chunk, instruction, model_path, n_ctx=4096, max_tokens=1024, stream=stream)
                responses.append(response)
                tables = extract_markdown_tables(response)
                all_tables.extend(tables)
            if input("Extract HTML tables from page? (y/n): ").strip().lower() == "y":
                html_tables = extract_html_tables(html)
                all_tables.extend(html_tables)
            save_tables(all_tables, url, save_type, merge=merge_tables)
            # Optionally save raw LLM responses
            if input("Save raw LLM responses? (y/n): ").strip().lower() == "y":
                filename = url_to_filename(url, "llm.txt")
                with open(filename, "w", encoding="utf-8") as f:
                    for i, resp in enumerate(responses):
                        f.write(f"--- Chunk {i+1} ---\n{resp}\n\n")
                print(f"Raw LLM responses saved to {filename}")
            # Optionally save HTML
            if input("Save raw HTML? (y/n): ").strip().lower() == "y":
                filename = url_to_filename(url, "html")
                with open(filename, "w", encoding="utf-8") as f:
                    f.write(html)
                print(f"HTML saved to {filename}")
        except Exception as e:
            print(f"Error processing {url}: {e}")
            log_error(f"{url}: {e}")